{
  "pipeline": "atomic_fact_scorer",
  "model": "gpt-4o-mini",
  "total_examples": 549,
  "evaluated_count": 10,
  "total_time_seconds": 74.20894193649292,
  "time_per_example_seconds": 7.420894193649292,
  "metrics_by_threshold": {
    "threshold_0.5": {
      "threshold": 0.5,
      "accuracy": 0.4,
      "precision": 0.14285714285714285,
      "recall": 1.0,
      "f1_score": 0.25,
      "true_positives": 1,
      "true_negatives": 3,
      "false_positives": 6,
      "false_negatives": 0,
      "mae": 0.45559918839330604,
      "rmse": 0.5443143618540243,
      "pearson_correlation": 0.056075313490244916,
      "pearson_p_value": 0.8777202366127803,
      "spearman_correlation": -0.14589733048321551,
      "spearman_p_value": 0.6875567115411831
    },
    "threshold_0.6": {
      "threshold": 0.6,
      "accuracy": 0.4,
      "precision": 0.0,
      "recall": 0.0,
      "f1_score": 0.0,
      "true_positives": 0,
      "true_negatives": 4,
      "false_positives": 6,
      "false_negatives": 0,
      "mae": 0.45559918839330604,
      "rmse": 0.5443143618540243,
      "pearson_correlation": 0.056075313490244916,
      "pearson_p_value": 0.8777202366127803,
      "spearman_correlation": -0.14589733048321551,
      "spearman_p_value": 0.6875567115411831
    },
    "threshold_0.7": {
      "threshold": 0.7,
      "accuracy": 0.5,
      "precision": 0.0,
      "recall": 0.0,
      "f1_score": 0.0,
      "true_positives": 0,
      "true_negatives": 5,
      "false_positives": 5,
      "false_negatives": 0,
      "mae": 0.45559918839330604,
      "rmse": 0.5443143618540243,
      "pearson_correlation": 0.056075313490244916,
      "pearson_p_value": 0.8777202366127803,
      "spearman_correlation": -0.14589733048321551,
      "spearman_p_value": 0.6875567115411831
    },
    "threshold_0.8": {
      "threshold": 0.8,
      "accuracy": 0.7,
      "precision": 0.0,
      "recall": 0.0,
      "f1_score": 0.0,
      "true_positives": 0,
      "true_negatives": 7,
      "false_positives": 3,
      "false_negatives": 0,
      "mae": 0.45559918839330604,
      "rmse": 0.5443143618540243,
      "pearson_correlation": 0.056075313490244916,
      "pearson_p_value": 0.8777202366127803,
      "spearman_correlation": -0.14589733048321551,
      "spearman_p_value": 0.6875567115411831
    },
    "threshold_0.9": {
      "threshold": 0.9,
      "accuracy": 0.8,
      "precision": 0.0,
      "recall": 0.0,
      "f1_score": 0.0,
      "true_positives": 0,
      "true_negatives": 8,
      "false_positives": 2,
      "false_negatives": 0,
      "mae": 0.45559918839330604,
      "rmse": 0.5443143618540243,
      "pearson_correlation": 0.056075313490244916,
      "pearson_p_value": 0.8777202366127803,
      "spearman_correlation": -0.14589733048321551,
      "spearman_p_value": 0.6875567115411831
    }
  },
  "best_threshold": "threshold_0.5",
  "best_metrics": {
    "threshold": 0.5,
    "accuracy": 0.4,
    "precision": 0.14285714285714285,
    "recall": 1.0,
    "f1_score": 0.25,
    "true_positives": 1,
    "true_negatives": 3,
    "false_positives": 6,
    "false_negatives": 0,
    "mae": 0.45559918839330604,
    "rmse": 0.5443143618540243,
    "pearson_correlation": 0.056075313490244916,
    "pearson_p_value": 0.8777202366127803,
    "spearman_correlation": -0.14589733048321551,
    "spearman_p_value": 0.6875567115411831
  },
  "per_example_results": [
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 1,
      "topic": "Doug Sheehan",
      "ground_truth_accuracy": 0.2222222222222222,
      "predicted_score": 0.3076923076923077,
      "error_magnitude": 0.0854700854700855,
      "num_facts": 13,
      "supported_facts": 4
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 2,
      "topic": "Taral Hicks",
      "ground_truth_accuracy": 0.5238095238095238,
      "predicted_score": 0.9444444444444444,
      "error_magnitude": 0.4206349206349206,
      "num_facts": 18,
      "supported_facts": 17
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 3,
      "topic": "Chief Jones",
      "ground_truth_accuracy": 0.0,
      "predicted_score": 0.9166666666666666,
      "error_magnitude": 0.9166666666666666,
      "num_facts": 13,
      "supported_facts": 11
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 4,
      "topic": "Joey D. Vieira",
      "ground_truth_accuracy": 0.125,
      "predicted_score": 0.058823529411764705,
      "error_magnitude": 0.0661764705882353,
      "num_facts": 17,
      "supported_facts": 1
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 5,
      "topic": "Lanny Flaherty",
      "ground_truth_accuracy": 0.19230769230769232,
      "predicted_score": 0.3,
      "error_magnitude": 0.10769230769230767,
      "num_facts": 20,
      "supported_facts": 6
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 6,
      "topic": "Chaim Malinowitz",
      "ground_truth_accuracy": 0.1,
      "predicted_score": 0.7647058823529411,
      "error_magnitude": 0.6647058823529411,
      "num_facts": 17,
      "supported_facts": 13
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 7,
      "topic": "Marianne McAndrew",
      "ground_truth_accuracy": 0.35294117647058826,
      "predicted_score": 0.6428571428571429,
      "error_magnitude": 0.28991596638655465,
      "num_facts": 14,
      "supported_facts": 9
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 8,
      "topic": "Focus...",
      "ground_truth_accuracy": 0.0,
      "predicted_score": 0.75,
      "error_magnitude": 0.75,
      "num_facts": 20,
      "supported_facts": 15
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 9,
      "topic": "Gerhard Fischer (inventor)",
      "ground_truth_accuracy": 0.07692307692307693,
      "predicted_score": 0.8571428571428571,
      "error_magnitude": 0.7802197802197801,
      "num_facts": 14,
      "supported_facts": 12
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 10,
      "topic": "Carolina Portesi Peroni",
      "ground_truth_accuracy": 0.058823529411764705,
      "predicted_score": 0.5333333333333333,
      "error_magnitude": 0.4745098039215686,
      "num_facts": 15,
      "supported_facts": 8
    }
  ]
}