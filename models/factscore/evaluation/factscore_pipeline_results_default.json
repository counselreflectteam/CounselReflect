{
  "pipeline": "atomic_fact_scorer",
  "model": "gpt-4o-mini",
  "mode": "default",
  "total_examples": 549,
  "evaluated_count": 10,
  "total_time_seconds": 64.47456216812134,
  "time_per_example_seconds": 6.447456216812133,
  "metrics_by_threshold": {
    "threshold_0.5": {
      "threshold": 0.5,
      "accuracy": 0.4,
      "precision": 0.14285714285714285,
      "recall": 1.0,
      "f1_score": 0.25,
      "true_positives": 1,
      "true_negatives": 3,
      "false_positives": 6,
      "false_negatives": 0,
      "mae": 0.4985356963298139,
      "rmse": 0.6006734570077924,
      "pearson_correlation": -0.04092946542748474,
      "pearson_p_value": 0.9106166315985564,
      "spearman_correlation": -0.3353658536585366,
      "spearman_p_value": 0.3434779895817571
    },
    "threshold_0.6": {
      "threshold": 0.6,
      "accuracy": 0.4,
      "precision": 0.0,
      "recall": 0.0,
      "f1_score": 0.0,
      "true_positives": 0,
      "true_negatives": 4,
      "false_positives": 6,
      "false_negatives": 0,
      "mae": 0.4985356963298139,
      "rmse": 0.6006734570077924,
      "pearson_correlation": -0.04092946542748474,
      "pearson_p_value": 0.9106166315985564,
      "spearman_correlation": -0.3353658536585366,
      "spearman_p_value": 0.3434779895817571
    },
    "threshold_0.7": {
      "threshold": 0.7,
      "accuracy": 0.4,
      "precision": 0.0,
      "recall": 0.0,
      "f1_score": 0.0,
      "true_positives": 0,
      "true_negatives": 4,
      "false_positives": 6,
      "false_negatives": 0,
      "mae": 0.4985356963298139,
      "rmse": 0.6006734570077924,
      "pearson_correlation": -0.04092946542748474,
      "pearson_p_value": 0.9106166315985564,
      "spearman_correlation": -0.3353658536585366,
      "spearman_p_value": 0.3434779895817571
    },
    "threshold_0.8": {
      "threshold": 0.8,
      "accuracy": 0.7,
      "precision": 0.0,
      "recall": 0.0,
      "f1_score": 0.0,
      "true_positives": 0,
      "true_negatives": 7,
      "false_positives": 3,
      "false_negatives": 0,
      "mae": 0.4985356963298139,
      "rmse": 0.6006734570077924,
      "pearson_correlation": -0.04092946542748474,
      "pearson_p_value": 0.9106166315985564,
      "spearman_correlation": -0.3353658536585366,
      "spearman_p_value": 0.3434779895817571
    },
    "threshold_0.9": {
      "threshold": 0.9,
      "accuracy": 0.7,
      "precision": 0.0,
      "recall": 0.0,
      "f1_score": 0.0,
      "true_positives": 0,
      "true_negatives": 7,
      "false_positives": 3,
      "false_negatives": 0,
      "mae": 0.4985356963298139,
      "rmse": 0.6006734570077924,
      "pearson_correlation": -0.04092946542748474,
      "pearson_p_value": 0.9106166315985564,
      "spearman_correlation": -0.3353658536585366,
      "spearman_p_value": 0.3434779895817571
    }
  },
  "best_threshold": "threshold_0.5",
  "best_metrics": {
    "threshold": 0.5,
    "accuracy": 0.4,
    "precision": 0.14285714285714285,
    "recall": 1.0,
    "f1_score": 0.25,
    "true_positives": 1,
    "true_negatives": 3,
    "false_positives": 6,
    "false_negatives": 0,
    "mae": 0.4985356963298139,
    "rmse": 0.6006734570077924,
    "pearson_correlation": -0.04092946542748474,
    "pearson_p_value": 0.9106166315985564,
    "spearman_correlation": -0.3353658536585366,
    "spearman_p_value": 0.3434779895817571
  },
  "per_example_results": [
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 1,
      "topic": "Doug Sheehan",
      "ground_truth_accuracy": 0.2222222222222222,
      "predicted_score": 0.3076923076923077,
      "error_magnitude": 0.0854700854700855,
      "num_facts": 13,
      "supported_facts": 4
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 2,
      "topic": "Taral Hicks",
      "ground_truth_accuracy": 0.5238095238095238,
      "predicted_score": 1.0,
      "error_magnitude": 0.47619047619047616,
      "num_facts": 19,
      "supported_facts": 19
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 3,
      "topic": "Chief Jones",
      "ground_truth_accuracy": 0.0,
      "predicted_score": 0.9166666666666666,
      "error_magnitude": 0.9166666666666666,
      "num_facts": 13,
      "supported_facts": 11
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 4,
      "topic": "Joey D. Vieira",
      "ground_truth_accuracy": 0.125,
      "predicted_score": 0.058823529411764705,
      "error_magnitude": 0.0661764705882353,
      "num_facts": 17,
      "supported_facts": 1
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 5,
      "topic": "Lanny Flaherty",
      "ground_truth_accuracy": 0.19230769230769232,
      "predicted_score": 0.3,
      "error_magnitude": 0.10769230769230767,
      "num_facts": 20,
      "supported_facts": 6
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 6,
      "topic": "Chaim Malinowitz",
      "ground_truth_accuracy": 0.1,
      "predicted_score": 0.7647058823529411,
      "error_magnitude": 0.6647058823529411,
      "num_facts": 17,
      "supported_facts": 13
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 7,
      "topic": "Marianne McAndrew",
      "ground_truth_accuracy": 0.35294117647058826,
      "predicted_score": 0.5714285714285714,
      "error_magnitude": 0.21848739495798314,
      "num_facts": 14,
      "supported_facts": 8
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 8,
      "topic": "Focus...",
      "ground_truth_accuracy": 0.0,
      "predicted_score": 0.8,
      "error_magnitude": 0.8,
      "num_facts": 20,
      "supported_facts": 16
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 9,
      "topic": "Gerhard Fischer (inventor)",
      "ground_truth_accuracy": 0.07692307692307693,
      "predicted_score": 0.7857142857142857,
      "error_magnitude": 0.7087912087912087,
      "num_facts": 14,
      "supported_facts": 11
    },
    {
      "source_file": "InstructGPT.jsonl",
      "line_number": 10,
      "topic": "Carolina Portesi Peroni",
      "ground_truth_accuracy": 0.058823529411764705,
      "predicted_score": 1.0,
      "error_magnitude": 0.9411764705882353,
      "num_facts": 15,
      "supported_facts": 15
    }
  ]
}